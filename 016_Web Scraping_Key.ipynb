{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 016_Web Scraping_Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Webpages and Parsing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping allows us to gather large amounts of data from the web quickly instead of opening the data source page by page and copying and pasting it into a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples where web scraping can come in handy:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To save a table from a Wikipedia page\n",
    "* To get a list of reviews from a movie site\n",
    "* To get a list of trending news stories \n",
    "* To get a listing of real-estate properties in a particular area\n",
    "* etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Requests Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the easiest way to download a web page in Python is to use the Requests library.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requests module doesn't come with Python, so you have to install it with pip install command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time you want to use the requests module, all you have to do is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Scraping HackerNews Front Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Broucke S., Baesens B. (2018). Practical Web Scraping for Data Science. Github page: https://github.com/Macuyiko/webscrapingfordatascience/blob/master/python-examples/hacker-news/without_api.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses requests and Beautiful Soup to scrape the Hacker News front page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to create a list with the link and title of each of 30 articles on the page, as well as the article's score and the number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.ycombinator.com/news'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the requests.get() function and pass our URL to it. That's what makes it so simple: we are passing the URL in the format that we are accustomed to and the requests.get() function will format a proper HTTP request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the contents of the page, use r.text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the page that we just visited are rendered to us via HTML, a markup language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to print out all of it, you can do a slice. Sometimes printing out the beginning is enough to see the tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text[:2001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raise_for_status() method is a way to ensure that you are notified if something goes wrong. You can wrap it in try/except clause to see the exception right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    r.raise_for_status()\n",
    "except Exception as exc:\n",
    "    print('There was a problem: %s' % (exc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see \"200\" when you run the following line, it means everything went as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the status in plain English using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line returns the HTTP request headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to learn how to extract information from our HTML string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you downloaded a web page using the requests library, you can parse it using the BeautifulSoup module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    ">\"The Beautiful Soup library was named after a Lewis Carroll poem bearing the same name from \"Alice's Adventures in Wonderland.\" In the tale, the poem is sung by a character called the \"Mock Turtle\" and goes as follows: \"Beautiful Soup, so rich and green,// Waiting in a hot tureen!// Who for such dainties would not stoop?// Soup of the evening, beautiful Soup!\". Just like in the story, Beautiful Soup tries to organize complexity: it helps to parse, structure and organize the oftentimes very messy web by fixing bad HTML and presenting us with an easy-to-work-with Python structure.\"\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a BeautifulSoup object. If you already have an HTML page contained in a string (as we have), this is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beautiful Soup library depends on an HTML parser to perform most of the parsing work. We'll be using 'html.parser' that doesn't require any additional installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(r.text, 'html.parser') #Creating a BeautifulSoup object from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a BeautifulSoup object, you can use its methods to locate specific parts of an HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main methods to locate elements within HTML: **find** and **find_all**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* find (name, attrs, recursive, string, **keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* find_all (name, attrs, recursive, string, limit, **keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Beautiful Soup also recognizes \"camelCaps\" capitalization. So instead of find_all, sometimes you'll see findAll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the arguments that find and find_all take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">find (**name**, attrs, recursive, string, keywords);\n",
    "<br> The name argument defines the tags you wish to \"find\" on the page. You can pass a string, or a list of tags. Leaving this argument as an empty string simply selects all elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">find (name, **attrs**, recursive, string, keywords);\n",
    "<br>The attrs argument takes a Python dictionary of attributes and matches HTML elements that match those attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">find_all (name, attrs, recursive, string, **limit**, keywords);\n",
    "<br>The limit argument is only used in the find_all method and can be used to limit the number of elements that are retrieved. Note that find is functionally equivalent to calling find_all with the limit set to 1, with the exception that the former returns the retrieved element directly, and that the latter will always return a list of items, even if it just contains a single element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from find and find_all, there are also a number of other methods for searching the HTML tree, which are very similar to find and find_all. The difference is that they will search different parts of the HTML tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **find_parent** and **find_parents** work their way up the tree, looking at a tag's parents using its parents attribute. Remember that find and find_all work their way down the tree, looking at a tag's descendants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **find_next_sibling** and **find_next_siblings** will iterate and match a tag's siblings using next_siblings attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of available methods and parameters, the official Beautiful Soup documentation is available at https://www.crummy.com/software/BeautifulSoup/bs4/doc/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertext Markup Language (HTML) Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML defines how a web page is structured and formatted. Its main building blocks are called tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us find the tags that we need, we can use browser tools. \n",
    "<br>Nearly every browser will have a tab titled Elements or HTML. \n",
    "<br>In Chrome and Firefox, you can right click on an element on the page and select Inspect Element. \n",
    "<br>For Internet Explorer, you need to open the Developer toolbar by pressing F12. \n",
    "<br>Then you can select items by clicking Ctrl + B .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tags that have content come in pairs, others do not. Tags are enclosed in angled brackets. Here are some of the most commonly used tags:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<p>...</p> to enclose a paragraph\n",
    "<table>...</table> to start a table block, inside <tr>...</tr> is used for the rows and <td>...</td> for the cells\n",
    "<h1>...</h1> to <h6>...</h6> for headers\n",
    "<div>...</div> to indicate a \"division\" inside an HTML document (to group elements)\n",
    "<a>...</a> for hyperlinks\n",
    "<br> to set a line break\n",
    "<img> for images\n",
    "<ul>...</ul>, <ol>...</ol> for lists (unordered and ordered respectively), inside, <li>...</li> is used for each list item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 (Continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, our goal is to create a list of links to each of the 30 articles mentioned on the page, record their titles, number of points and number of comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using regular expression to find the comments section, so we need to import *re* module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.ycombinator.com/news'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a list to store our list of articles and other findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in html_soup.find_all('tr', class_='athing'):\n",
    "    item_a = item.find('a', class_='storylink')\n",
    "    item_link = item_a.get('href') if item_a else None\n",
    "    item_text = item_a.get_text(strip=True) if item_a else None\n",
    "    next_row = item.find_next_sibling('tr')\n",
    "    item_score = next_row.find('span', class_='score')\n",
    "    item_score = item_score.get_text(strip=True) if item_score else '0 points'\n",
    "    # We use regex here to find the correct element\n",
    "    item_comments = next_row.find('a', text=re.compile('\\d+(&nbsp;|\\s)comment(s?)'))\n",
    "    item_comments = item_comments.get_text(strip=True).replace('\\xa0', ' ') \\\n",
    "                        if item_comments else '0 comments'\n",
    "    \n",
    "    articles.append({\n",
    "        'link' : item_link,\n",
    "        'title' : item_text,\n",
    "        'score' : item_score,\n",
    "        'comments' : item_comments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice. Scraping analytics.usa.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Pierson, L. Python for Data Science Essential Training (2019). LinkedIn Learning. https://www.linkedin.com/learning/python-for-data-science-essential-training-part-1/web-scraping-in-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to get a list of all web links included in the https://analytics.usa.gov/ webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a BeautifulSoup object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://analytics.usa.gov/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all web links included in the webpage. To do that, use a loop and find_all function to find and print all a tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in html_soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in html_soup.find_all('a', attrs = {'href': re.compile(\"^http\")}):\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, save the links in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"parsed_data.txt\", \"w\")\n",
    "for link in html_soup.find_all('a', attrs = {'href': re.compile(\"^http\")}):\n",
    "    soup_link = str(link)\n",
    "    print(soup_link)\n",
    "    file.write(soup_link)\n",
    "file.flush()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: List of Game of Thrones Episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll be working with Game of Thrones Wikipedia page that has a number of tables listing the episodes with their directors, writers, air dates, and number of viewers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fetch all of this data using what we have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use requests module to import the URL\n",
    "r = requests.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_contents = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a BeautifulSoup object\n",
    "html_soup = BeautifulSoup(html_contents, 'html.parser')\n",
    "\n",
    "type(html_soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the episode tables on the page. What tag is used to define a table on this page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the episode tables distinguished from all other tables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every table, we first want to retrieve the headers to use as keys in a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first h1 tag\n",
    "first_h1 = html_soup.find('h1')\n",
    "\n",
    "print(first_h1.name)     # h1\n",
    "print(first_h1.contents) # ['List of ', [...], ' episodes']\n",
    "\n",
    "print(str(first_h1))\n",
    "# Prints out: <h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">List of\n",
    "#             <i>Game of Thrones</i> episodes</h1>\n",
    "\n",
    "print(first_h1.text)       # List of Game of Thrones episodes\n",
    "print(first_h1.get_text()) # Does the same\n",
    "\n",
    "print(first_h1.attrs)\n",
    "# Prints out: {'id': 'firstHeading', 'class': ['firstHeading'], 'lang': 'en'}\n",
    "\n",
    "print(first_h1.attrs['id']) # firstHeading\n",
    "print(first_h1['id'])       # Does the same\n",
    "print(first_h1.get('id'))   # Does the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a list to store our episode list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the episode tables on the page. Note how they're all defined by means of a *table* tag. However, the page also contains tables we do not want to include. Some further investigation leads us to a solution: all the episode tables have \"wikiepisodetable\" as a class, whereas the other tables do not. You'll often have to puzzle your way through a page first before coming up with a solid approach. In many cases, you'll have to perform multiple find and find_all iterations before ending up where you want to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all episode tables and store them in a variable ep_tables\n",
    "ep_tables = html_soup.find_all('table', class_='wikiepisodetable')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through every episode table. First, we create a list of headers like we did above. Next, we loop through all the rows (the *tr* tags), except for the first one (the header row). For each row, we loop through the *th* and *td* tags to extract the column values (the first column is wrapped inside of a *th* tag, the others in *td* tags, which is why we need to handle both). At the end of each row, we're ready to add a new entry to the \"episodes\" variable. To store each entry, we use a normal Python dictionary (episode_dict). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in ep_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    # Start by fetching the header cells from the first row to determine\n",
    "    # the field names\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    # Then go through all the rows except the first one\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        # And get the column cells, the first one being inside a th-tag\n",
    "        for col in row.find_all(['th','td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            episode_dict = {headers[i]: values[i] for i in\n",
    "            range(len(values))}\n",
    "            episodes.append(episode_dict)\n",
    "\n",
    "# Show the results\n",
    "for episode in episodes:\n",
    "    print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice. Scrape MPG to Compare Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to find MPG for 4 cars (Ford Escape 2019, Honda CRV 2019,Hyundai Santa Fe 2019, and Toyota Rav4 2019) on Kelly Blue Book site and store it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_links = [\"https://www.kbb.com/ford/escape/2019/s/?vehicleid=439781&intent=buy-new\", \"https://www.kbb.com/honda/cr-v/2019/lx/?vehicleid=439688&intent=buy-new\", 'https://www.kbb.com/hyundai/santa-fe/2019/24-se/?vehicleid=436474&intent=buy-new', 'https://www.kbb.com/toyota/rav4/2019/le/?vehicleid=440064&intent=buy-new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tag is used to define the MPG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty list and fill it with the data fetched from the website. Use requests, BeautifulSoup, and a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for i in list_of_links:\n",
    "    r = requests.get(i)\n",
    "    html_contents = r.text\n",
    "    html_soup = BeautifulSoup(html_contents, 'html.parser')\n",
    "    fuel_economy = html_soup.find('p', {\"class\": \"mpg-value\"})\n",
    "    #print(fuel_economy)\n",
    "    contents.append(fuel_economy)\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents1 = []\n",
    "for i in contents:\n",
    "    mpg = i.contents\n",
    "    mpg = str(mpg)\n",
    "    contents1.append(mpg)\n",
    "    \n",
    "print(contents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "car_list = ['Ford Escape 2019: ', 'Honda CRV 2019: ', 'Hyundai Santa Fe 2019: ', 'Toyota Rav4 2019: ']\n",
    "for i in range(len(car_list)):\n",
    "    car_mpg = car_list[i] + contents1[i]\n",
    "    output_list.append(car_mpg)\n",
    "output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, save your results in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('output.csv', 'w', newline='') as output_file:\n",
    "    wr = csv.writer(output_file)\n",
    "    for item in output_list:\n",
    "        wr.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
